[{"title":"index","url":"/2025/03/11/index/","content":"<h1 id=\"hello-world\">hello world</h1>\n<p>This is the first blog!</p>\n"},{"title":"博客搭建日志","url":"/2025/03/11/blog-build-log/","content":"<h1 id=\"框架选择\">框架选择</h1>\n<p>之前搭建过一次博客，选用的是 hexo 框架，主要就是支持 markdown\n笔记比较方便，可以本地静态生成，然后送到 github 上之后只需要 github\nactions 里面放一个静态部署到 githup pages 的工作流即可。</p>\n<p>所以这次还是选择 hexo + next\n来搭建博客。找工作是真的难，什么都要准备好，搭建博客也是比较耗费时间的，因为需要调整样式还有踩坑。</p>\n<p>比如深色模式还有博客背景动效，一些内容的填充等等。域名的解析和配置，也都比较重要。</p>\n<h1 id=\"踩坑记录\">踩坑记录</h1>\n<ol type=\"1\">\n<li>nodejs 安装的包如果存在可执行文件，则放在 <code>.bin</code>\n目录下，所以 hexo 框架是可以局部安装的，而不一定需要安装到 nodejs 的全局\n<code>.bin</code> 目录，方便管理</li>\n<li>默认的 hexo-github-deploy 插件，并不会生成静态部署的 github\nactions，所以如果是手动在网页的 git\n仓库上操作部署执行，则下一次将本地的文章页面仓库推送到远端的时候，又会把\ngithub actions 覆盖掉，变成空。因此需要在本地将 github 的静态页面部署\nactions 复制一份放到 <code>.github/workflows</code> 目录下，这样\n<code>hexo d</code>\n部署本地页面到远端仓库的时候才会自动触发，将更新后的页面部署到 github\npages</li>\n<li>vercel 用于克隆托管 github pages 仓库，因为 github\n不开梯子国内是无法访问到的，但是 cloudflare 国内是可以直连的，vercel\n应该也是，所以可以在 cloudflare 上配置域名解析到 vercel\n的页面仓库，这样国内也可以直接访问博客啦</li>\n<li>Next 主题现在已经直接将 <code>darkmode.js</code>\n作为神色主题切换的选项了，不需要自己将 <code>darkmode.js</code>\n下载到主题的 <code>source/lib</code>\n目录下做本地依赖（有可能是插件帮我们做了这件事情）</li>\n<li><code>darkmode.js</code> 的深色模式并没有同步 next\n主题代码块深色模式，所以还是不好看，可以默认把代码块设置成深色模式，这样切换了深色模式也比较匹配</li>\n<li>markdown\n笔记内的图片链接，可能需要额外写一个脚本转换成图床的链接？或者不知道可不可以把笔记仓库的图像文件夹直接复制到\nhexo 下，但是估计不行，太多笔记了，图像文件估计都要几个 GB\n，不太适合同步到博客文章仓库，最好还是本地写笔记的时候上传到图床，然后博客发布的时候把本地的相对图像链接转成图床链接</li>\n</ol>\n","categories":["日志"]},{"title":"公式渲染测试","url":"/2025/03/11/formula-test/","content":"<p>转载：<a\nhref=\"https://zhuanlan.zhihu.com/p/39363869\">浅析机器学习：线性回归\n&amp; 逻辑回归 - 知乎</a></p>\n<h1 id=\"逻辑回归和线性回归\">逻辑回归和线性回归</h1>\n<h3 id=\"应用场景\">1. 应用场景</h3>\n<ul>\n<li><strong>线性回归</strong>：用于预测连续值，如房价、销售额等</li>\n<li><strong>逻辑回归</strong>：用于分类问题，特别是二分类，如判断邮件是否为垃圾邮件</li>\n</ul>\n<h3 id=\"输出类型\">2. 输出类型</h3>\n<ul>\n<li><strong>线性回归</strong>：输出为连续值，范围无限制</li>\n<li><strong>逻辑回归</strong>：输出为概率值，通过Sigmoid函数映射到 [0,\n1] 区间</li>\n</ul>\n<h3 id=\"模型函数\">3. 模型函数</h3>\n<ul>\n<li><strong>线性回归</strong>：使用线性函数<span class=\"math inline\">\\(y\n= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n\nx_n\\)</span></li>\n<li><strong>逻辑回归</strong>：使用逻辑函数<span class=\"math inline\">\\(p\n= \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots +\n\\beta_n x_n)}}\\)</span></li>\n</ul>\n<h3 id=\"损失函数\">4. 损失函数</h3>\n<ul>\n<li><strong>线性回归</strong>：通常使用均方误差（MSE）</li>\n<li><strong>逻辑回归</strong>：使用对数损失函数（Log Loss）</li>\n</ul>\n<h3 id=\"参数估计\">5. 参数估计</h3>\n<ul>\n<li><strong>线性回归</strong>：通过最小二乘法估计参数</li>\n<li><strong>逻辑回归</strong>：通过最大似然估计法估计参数</li>\n</ul>\n<h3 id=\"假设条件\">6. 假设条件</h3>\n<ul>\n<li><strong>线性回归</strong>：假设误差项服从正态分布，且自变量与因变量呈线性关系</li>\n<li><strong>逻辑回归</strong>：不要求误差项正态分布，但假设因变量与自变量的对数几率呈线性关系</li>\n</ul>\n<h3 id=\"多重共线性\">7. 多重共线性</h3>\n<ul>\n<li><strong>线性回归</strong>：对多重共线性敏感，可能导致参数估计不稳定</li>\n<li><strong>逻辑回归</strong>：对多重共线性也有一定敏感性，但影响相对较小</li>\n</ul>\n<h3 id=\"正则化\">8. 正则化</h3>\n<ul>\n<li><strong>线性回归</strong>：可通过L1或L2正则化防止过拟合</li>\n<li><strong>逻辑回归</strong>：同样支持L1或L2正则化，常用于高维数据</li>\n</ul>\n<h3 id=\"总结\">总结</h3>\n<ul>\n<li><strong>线性回归</strong>：预测连续值，使用线性函数和均方误差</li>\n<li><strong>逻辑回归</strong>：用于分类，使用逻辑函数和对数损失函数</li>\n</ul>\n<p>根据具体问题选择合适的回归方法。</p>\n<h1 id=\"从线性回归推广到广义线性回归\">从线性回归推广到广义线性回归</h1>\n<p>上面我们得到了线性回归模型的数学原型，在数学上一个特例经常都是归属于一个更普遍或更一般的原型。让我们思考下面这两个回归模型：</p>\n<p><span class=\"math display\">\\[ y = \\beta X + \\epsilon \\]</span></p>\n<p><span class=\"math display\">\\[ \\ln y = \\beta X + \\epsilon\n\\]</span></p>\n<p>左边是我们之前得到的线性回归模型，右边是对数线性回归模型（Log-Linear\nRegression）。从等式的形式来看，对数线性回归与线性回归区别仅在于等式左部，形式依旧是线性回归，但实质上是完成了输入空间\n<span class=\"math inline\">\\(X\\)</span>\n到输出空间内的非线性映射。这里的对数函数 <span\nclass=\"math inline\">\\(\\ln(\\cdot)\\)</span>\n，将线性回归模型和真实观测联系起来。通俗地说，原本线性回归模型无法描述的非线性\n<span class=\"math inline\">\\(y\\)</span> ，套上了一个非线性函数 <span\nclass=\"math inline\">\\(\\ln(\\cdot)\\)</span> ，就可以描述对数形式的 <span\nclass=\"math inline\">\\(y\\)</span> 了。</p>\n<p><span class=\"math display\">\\[ y = g^{-1}(\\beta X + \\epsilon)\n\\]</span></p>\n<p>将以上两个式子综合，写成更一般的形式就是广义线性回归模型（GLM，Generalized\nLinear Model）了。这里的 <span class=\"math inline\">\\(g(\\cdot)\\)</span>\n，即 <span class=\"math inline\">\\(\\ln(\\cdot)\\)</span>\n，是一个单调可微函数，称为联系函数（Link\nFunction）。显然，前面的线性回归和对数回归都是广义线性回归的特例，根据联系函数的不同，以不同的方式映射，可以是对数，可以是指数，也可以是其他更复杂的函数。</p>\n<h1\nid=\"理解最小二乘法中的凸函数性质与二阶导零点\">理解最小二乘法中的凸函数性质与二阶导零点</h1>\n<p>线性回归使用的参数估计方法是“最小二乘法”，而最小二乘法的实质是试图找到一条直线，使得所有样本点和拟合直线的欧氏距离之和最小。这个寻找的过程简单描述就是：根据凸函数的性质，求其关于β和ε的二阶导的零点。这句话我不是很懂，主要是后半部分那个凸函数性质二阶导零点就是最优的参数点，怎么理解？</p>\n<p>最小二乘法的目标是通过最小化误差平方和来找到最优参数。具体来说：</p>\n<ol type=\"1\">\n<li><p><strong>误差平方和</strong>： 对于线性回归模型 <span\nclass=\"math inline\">\\(y = X\\beta + \\epsilon\\)</span>\n，误差平方和（SSE）为： <span class=\"math display\">\\[\nSSE = \\sum_{i=1}^{n} (y_i - X_i \\beta)^2\n\\]</span> 其中，<span class=\"math inline\">\\(y_i\\)</span> 是实际值，<span\nclass=\"math inline\">\\(X_i \\beta\\)</span> 是预测值。</p></li>\n<li><p><strong>凸函数性质</strong>： SSE 是关于参数<span\nclass=\"math inline\">\\(\\beta\\)</span>的二次函数，且其 Hessian\n矩阵（二阶导数矩阵）是正定的，因此 SSE\n是一个凸函数。凸函数的性质保证了其局部最小值就是全局最小值。</p></li>\n<li><p><strong>求导与极值</strong>： 为了找到最小值，我们对 SSE\n关于<span class=\"math inline\">\\(\\beta\\)</span>求导并令导数为零： <span\nclass=\"math display\">\\[\n\\frac{\\partial SSE}{\\partial \\beta} = -2X^T(y - X\\beta) = 0\n\\]</span> 解这个方程可以得到最优参数估计： <span class=\"math display\">\\[\n\\hat{\\beta} = (X^T X)^{-1} X^T y\n\\]</span></p></li>\n<li><p><strong>二阶导与凸函数</strong>： 二阶导数（Hessian 矩阵）为：\n<span class=\"math display\">\\[\n\\frac{\\partial^2 SSE}{\\partial \\beta^2} = 2X^T X\n\\]</span> 由于<span class=\"math inline\">\\(X^T X\\)</span>是正定的，SSE\n是凸函数，因此导数为零的点就是全局最小值点。</p></li>\n</ol>\n<h3 id=\"总结-1\">总结</h3>\n<ul>\n<li><strong>凸函数性质</strong>：保证了误差平方和函数只有一个全局最小值。</li>\n<li><strong>二阶导零点</strong>：通过求导并令导数为零，可以找到这个最小值点，即最优参数估计。</li>\n</ul>\n<p>这个过程利用了凸函数的性质，确保了我们找到的参数估计是最优的。</p>\n<h1\nid=\"什么是二阶导数矩阵为什么二阶导数会形成矩阵\">什么是二阶导数矩阵？为什么二阶导数会形成矩阵？</h1>\n<h3 id=\"二阶导数矩阵hessian-矩阵\">二阶导数矩阵（Hessian 矩阵）</h3>\n<p>在多元函数中，二阶导数矩阵（Hessian\n矩阵）是一个非常重要的概念，它描述了函数在各个方向上的曲率。具体来说：</p>\n<ol type=\"1\">\n<li><p><strong>定义</strong>： 对于一个多元函数 <span\nclass=\"math inline\">\\(f(x_1, x_2, \\dots, x_n)\\)</span>，其 Hessian 矩阵\n$H $ 是一个 <span class=\"math inline\">\\(n\\times n\\)</span>\n的矩阵，其中每个元素是函数的二阶偏导数： <span class=\"math display\">\\[\nH_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}\n\\]</span> 因此，Hessian 矩阵可以表示为： <span class=\"math display\">\\[\nH = \\begin{bmatrix}\n\\frac{\\partial^2 f}{\\partial x_1^2} &amp; \\frac{\\partial^2 f}{\\partial\nx_1 \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_1\n\\partial x_n} \\\\\n\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} &amp; \\frac{\\partial^2\nf}{\\partial x_2^2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_2\n\\partial x_n} \\\\\n\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\\n\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} &amp; \\frac{\\partial^2\nf}{\\partial x_n \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2\nf}{\\partial x_n^2}\n\\end{bmatrix}\n\\]</span></p></li>\n<li><p><strong>为什么导数会形成矩阵</strong>：</p>\n<ul>\n<li><strong>一阶导数</strong>：对于多元函数，一阶导数是一个向量，称为梯度向量。梯度向量的每个元素是函数对各个变量的偏导数。</li>\n<li><strong>二阶导数</strong>：二阶导数描述的是梯度向量的变化率。由于梯度向量本身是一个向量，其变化率（即二阶导数）自然形成一个矩阵。这个矩阵就是\nHessian 矩阵。</li>\n</ul></li>\n</ol>\n<h3 id=\"例子\">例子</h3>\n<p>考虑一个二元函数 <span class=\"math inline\">\\(f(x, y)\\)</span> ，其\nHessian 矩阵为： <span class=\"math display\">\\[\nH = \\begin{bmatrix}\n\\frac{\\partial^2 f}{\\partial x^2} &amp; \\frac{\\partial^2 f}{\\partial x\n\\partial y} \\\\\n\\frac{\\partial^2 f}{\\partial y \\partial x} &amp; \\frac{\\partial^2\nf}{\\partial y^2}\n\\end{bmatrix}\n\\]</span></p>\n<h3 id=\"作用\">作用</h3>\n<ol type=\"1\">\n<li><strong>凸性判断</strong>：\n<ul>\n<li>如果 Hessian 矩阵是正定的，则函数在该点是凸的，存在局部最小值。</li>\n<li>如果 Hessian 矩阵是负定的，则函数在该点是凹的，存在局部最大值。</li>\n<li>如果 Hessian 矩阵是不定的，则函数在该点是鞍点。</li>\n</ul></li>\n<li><strong>优化算法</strong>：\n<ul>\n<li>在优化问题中，Hessian\n矩阵用于牛顿法等二阶优化算法，以更快地收敛到最优解。</li>\n</ul></li>\n</ol>\n<h3 id=\"总结-2\">总结</h3>\n<ul>\n<li><strong>二阶导数矩阵</strong>：描述了多元函数在各个方向上的曲率。</li>\n<li><strong>形成矩阵的原因</strong>：因为一阶导数（梯度）是一个向量，其变化率（二阶导数）自然形成一个矩阵。</li>\n</ul>\n<p>理解 Hessian\n矩阵有助于深入分析多元函数的性质，并在优化问题中找到最优解。</p>\n<h1 id=\"混合二阶偏导\">混合二阶偏导</h1>\n<p>二阶导数可以先对其中一个变量求偏导然后第二阶的时候对另外一个变量求偏导？我以为是对同一个变量求两次倒数？?</p>\n<h3 id=\"二阶偏导数的理解\">二阶偏导数的理解</h3>\n<p>在多元函数中，二阶偏导数确实可以涉及对不同的变量求导。具体来说：</p>\n<ol type=\"1\">\n<li><strong>二阶偏导数的定义</strong>： 对于一个多元函数 <span\nclass=\"math inline\">\\(f(x_1, x_2, \\dots,\nx_n)\\)</span>，二阶偏导数有两种形式：\n<ul>\n<li><strong>纯二阶偏导数</strong>：对同一个变量求两次偏导数，例如 <span\nclass=\"math inline\">\\(\\frac{\\partial^2 f}{\\partial x_i^2}\\)</span></li>\n<li><strong>混合二阶偏导数</strong>：先对一个变量求偏导数，再对另一个变量求偏导数，例如\n<span class=\"math inline\">\\(\\frac{\\partial^2 f}{\\partial x_i \\partial\nx_j}\\)</span></li>\n</ul></li>\n<li><strong>例子</strong>： 考虑一个二元函数 <span\nclass=\"math inline\">\\(f(x, y)\\)</span>，其二阶偏导数包括：\n<ul>\n<li>纯二阶偏导数：<span class=\"math inline\">\\(\\frac{\\partial^2\nf}{\\partial x^2}\\)</span> 和 <span\nclass=\"math inline\">\\(\\frac{\\partial^2 f}{\\partial y^2}\\)</span></li>\n<li>混合二阶偏导数：<span class=\"math inline\">\\(\\frac{\\partial^2\nf}{\\partial x \\partial y}\\)</span> 和 <span\nclass=\"math inline\">\\(\\frac{\\partial^2 f}{\\partial y \\partial\nx}\\)</span></li>\n</ul></li>\n<li><strong>混合偏导数的对称性</strong>： 在大多数情况下，如果函数 <span\nclass=\"math inline\">\\(f\\)</span>\n的二阶偏导数连续，则混合偏导数是相等的，即： <span\nclass=\"math display\">\\[\n\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial^2\nf}{\\partial y \\partial x}\n\\]</span> 这个性质称为 Schwarz 定理或 Clairaut 定理。</li>\n</ol>\n<h3 id=\"具体步骤\">具体步骤</h3>\n<ol type=\"1\">\n<li><p><strong>一阶偏导数</strong>： 首先计算函数 <span\nclass=\"math inline\">\\(f(x, y)\\)</span> 的一阶偏导数： <span\nclass=\"math display\">\\[\n\\frac{\\partial f}{\\partial x} \\quad \\text{和} \\quad \\frac{\\partial\nf}{\\partial y}\n\\]</span></p></li>\n<li><p><strong>二阶偏导数</strong>： 然后对一阶偏导数再次求偏导数：</p>\n<ul>\n<li>纯二阶偏导数： <span class=\"math display\">\\[\n\\frac{\\partial^2 f}{\\partial x^2} = \\frac{\\partial}{\\partial x} \\left(\n\\frac{\\partial f}{\\partial x} \\right)\n\\]</span> <span class=\"math display\">\\[\n\\frac{\\partial^2 f}{\\partial y^2} = \\frac{\\partial}{\\partial y} \\left(\n\\frac{\\partial f}{\\partial y} \\right)\n\\]</span></li>\n<li>混合二阶偏导数： <span class=\"math display\">\\[\n\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial}{\\partial y}\n\\left( \\frac{\\partial f}{\\partial x} \\right)\n\\]</span> <span class=\"math display\">\\[\n\\frac{\\partial^2 f}{\\partial y \\partial x} = \\frac{\\partial}{\\partial x}\n\\left( \\frac{\\partial f}{\\partial y} \\right)\n\\]</span></li>\n</ul></li>\n</ol>\n<h2 id=\"几何意义\">几何意义</h2>\n<p>以二维标准正态分布（高斯函数）的函数图形为例：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> mpl_toolkits.mplot3d <span class=\"keyword\">import</span> Axes3D</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义二维高斯函数</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">gaussian_2d</span>(<span class=\"params\">x, y, mu_x=<span class=\"number\">0</span>, mu_y=<span class=\"number\">0</span>, sigma_x=<span class=\"number\">1</span>, sigma_y=<span class=\"number\">1</span></span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.exp(-((x - mu_x)**<span class=\"number\">2</span> / (<span class=\"number\">2</span> * sigma_x**<span class=\"number\">2</span>) + (y - mu_y)**<span class=\"number\">2</span> / (<span class=\"number\">2</span> * sigma_y**<span class=\"number\">2</span>)))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建网格数据</span></span><br><span class=\"line\">x = np.linspace(-<span class=\"number\">5</span>, <span class=\"number\">5</span>, <span class=\"number\">100</span>)</span><br><span class=\"line\">y = np.linspace(-<span class=\"number\">5</span>, <span class=\"number\">5</span>, <span class=\"number\">100</span>)</span><br><span class=\"line\">x, y = np.meshgrid(x, y)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算高斯函数的值</span></span><br><span class=\"line\">z = gaussian_2d(x, y)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建 3D 图形</span></span><br><span class=\"line\">fig = plt.figure()</span><br><span class=\"line\">ax = fig.add_subplot(<span class=\"number\">111</span>, projection=<span class=\"string\">&#x27;3d&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制表面图</span></span><br><span class=\"line\">surf = ax.plot_surface(x, y, z, cmap=<span class=\"string\">&#x27;viridis&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 添加颜色条</span></span><br><span class=\"line\">fig.colorbar(surf)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设置坐标轴标签</span></span><br><span class=\"line\">ax.set_xlabel(<span class=\"string\">&#x27;X axis&#x27;</span>)</span><br><span class=\"line\">ax.set_ylabel(<span class=\"string\">&#x27;Y axis&#x27;</span>)</span><br><span class=\"line\">ax.set_zlabel(<span class=\"string\">&#x27;Z axis&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 显示图形</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"../../../images/Pasted%20image%2020250102150514.png\" /></p>\n<h3 id=\"纯二阶偏导数\">纯二阶偏导数</h3>\n<p>描绘了，当某个维度的变量取特定值时，比如 <span\nclass=\"math inline\">\\(X\\)</span> 变量取 <span\nclass=\"math inline\">\\(x=0\\)</span>\n时的这条横截面上的曲线的梯度变化速率情况，比如 <span\nclass=\"math inline\">\\(x=0\\)</span>\n时的横截面（以上图为例）就是波动最大的，因此其一阶导数变化速率也最大；当\n<span class=\"math inline\">\\(x=2\\)</span>\n时，其图形的横截面曲线展现出来较为平缓，波峰不明显，则其一阶导数反应出横截面曲线各个位置的梯度也比较平缓，二阶导数反映出来一阶导数的变化情况就更平缓了。</p>\n<h3 id=\"混合二阶偏导数\">混合二阶偏导数</h3>\n<p>描绘了由两个变量共同确定的某一点处的梯度变化速率的情况，注意这里是梯度变化速率，而不是因变量的变化速率，因变量变化速率等价于梯度，梯度变化速率才是二阶导描绘的。混合偏导是先对某一个变量求偏导，也就是确定了第一个变量决定的位置，然后再在第一个变量决定好的横截面曲线上由第二个变量决定位置。</p>\n<h3 id=\"总结-3\">总结</h3>\n<ul>\n<li><strong>二阶偏导数</strong>：包括纯二阶偏导数和混合二阶偏导数。</li>\n<li><strong>混合偏导数</strong>：先对一个变量求偏导数，再对另一个变量求偏导数。</li>\n<li><strong>对称性</strong>：在大多数情况下，混合偏导数是相等的。</li>\n</ul>\n<p>理解这些概念有助于更全面地分析多元函数的性质，并在优化问题中找到最优解。</p>\n<p><img src=\"../../../images/Pasted%20image%2020250102150033.png\" /></p>\n<p><strong>先求导的变量写在后面。</strong>\n这种记法是国际上公认的记法，包括 wiki 。</p>\n"},{"title":"Hello World","url":"/2025/03/11/hello-world/","content":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very\nfirst post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for\nmore info. If you get any problems when using Hexo, you can find the\nanswer in <a\nhref=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or\nyou can ask me on <a\nhref=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"quick-start\">Quick Start</h2>\n<h3 id=\"create-a-new-post\">Create a new post</h3>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a\nhref=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"run-server\">Run server</h3>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"generate-static-files\">Generate static files</h3>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a\nhref=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"deploy-to-remote-sites\">Deploy to remote sites</h3>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a\nhref=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n"},{"title":"删除 git 误提交的文件追踪","url":"/2025/03/12/git-cached-delete/","content":"<p>我不小心在我的 git 仓库下追踪了 ipynb 文件，但是我已经 commit\n了，再修改 .gitignore 也不生效，怎么办？</p>\n<p>你可以按照以下步骤解决问题：</p>\n<ol type=\"1\">\n<li><p><strong>从 Git 缓存中移除文件但保留本地文件：</strong>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">git <span class=\"built_in\">rm</span> --cached *.ipynb  <span class=\"comment\"># 移除所有.ipynb 文件（保留本地文件）</span></span><br></pre></td></tr></table></figure> 如果文件在子目录中，确保路径正确（例如\n<code>git rm --cached **/*.ipynb</code>，可能需要开启通配符支持\n<code>shopt -s globstar</code>）。</p></li>\n<li><p><strong>将 <code>.ipynb</code> 文件添加到\n<code>.gitignore</code>：</strong> <figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">echo</span> <span class=\"string\">&quot;*.ipynb&quot;</span> &gt;&gt; .gitignore  <span class=\"comment\"># 追加忽略规则到 .gitignore</span></span><br></pre></td></tr></table></figure></p></li>\n<li><p><strong>提交此次变更到仓库：</strong> <figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">git add .gitignore</span><br><span class=\"line\">git commit -m <span class=\"string\">&quot;Stop tracking .ipynb files and update .gitignore&quot;</span></span><br></pre></td></tr></table></figure></p></li>\n<li><p><strong>如果已推送过历史记录到远程仓库：</strong>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">git push origin 你的分支名  <span class=\"comment\"># 强制推送可能需要用 --force（谨慎操作）</span></span><br></pre></td></tr></table></figure></p></li>\n</ol>\n<hr />\n<h3 id=\"重要补充\">������ 重要补充：</h3>\n<ul>\n<li><strong>如果文件已存在于远程历史：</strong>\n上述操作仅会从新提交中移除文件，但历史提交中仍保留.ipynb文件。若需彻底清除（如敏感信息），需使用\n<code>git filter-repo</code> 或 <code>BFG</code> 等工具重写历史。</li>\n<li><strong>首次使用 <code>git filter-repo</code> 的示例：</strong>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">git filter-repo --path-glob <span class=\"string\">&#x27;*.ipynb&#x27;</span> --invert-paths</span><br></pre></td></tr></table></figure> 这会删除所有提交中的.ipynb文件（⚠️\n会改变提交哈希，影响协作）。</li>\n</ul>\n<hr />\n<p>完成后，你的.ipynb文件将不再被追踪且后续修改也不会出现在未暂存列表中。</p>\n","categories":["DevOps"]}]